{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.transforms import functional as vision_F\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.networks import build_resnet18\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.CIFAR10(root='dataset/',download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Dataset and Dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset='cifar100',\n",
    "        data_folder='dataset',\n",
    "        batch_size=256,\n",
    "        num_workers=8,\n",
    "        size=32\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.data_folder = data_folder\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.size = size\n",
    "\n",
    "opt = Config(\n",
    "    dataset='cifar100',\n",
    "    data_folder='dataset',\n",
    "    batch_size=256,\n",
    "    num_workers=8,\n",
    "    size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoCropTransform:\n",
    "    \"\"\"Create two crops of the same image\"\"\"\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return [self.transform(x), self.transform(x)]\n",
    "\n",
    "def set_loader_contrastive(opt, crop_only=False):\n",
    "    # construct data loader\n",
    "    if opt.dataset == 'cifar10':\n",
    "        mean = (0.4914, 0.4822, 0.4465)\n",
    "        std = (0.2023, 0.1994, 0.2010)\n",
    "    elif opt.dataset == 'cifar100':\n",
    "        mean = (0.5071, 0.4867, 0.4408)\n",
    "        std = (0.2675, 0.2565, 0.2761)\n",
    "    elif opt.dataset == 'path':\n",
    "        mean = eval(opt.mean)\n",
    "        std = eval(opt.std)\n",
    "    else:\n",
    "        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n",
    "    normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "\n",
    "    if crop_only == False:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size=opt.size, scale=(0.2, 1.)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomApply([\n",
    "                transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
    "            ], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    else:\n",
    "        train_transform = transforms.Compose([\n",
    "            # transforms.RandomResizedCrop(size=opt.size, scale=(0.2, 1.)),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            # normalize,\n",
    "        ])\n",
    "\n",
    "\n",
    "    if opt.dataset == 'cifar10':\n",
    "        train_dataset = datasets.CIFAR10(root=opt.data_folder,\n",
    "                                         transform=TwoCropTransform(train_transform),\n",
    "                                         download=True)\n",
    "    elif opt.dataset == 'cifar100':\n",
    "        train_dataset = datasets.CIFAR100(root=opt.data_folder,\n",
    "                                          transform=TwoCropTransform(train_transform),\n",
    "                                          download=True)\n",
    "    elif opt.dataset == 'path':\n",
    "        train_dataset = datasets.ImageFolder(root=opt.data_folder,\n",
    "                                            transform=TwoCropTransform(train_transform))\n",
    "    else:\n",
    "        raise ValueError(opt.dataset)\n",
    "\n",
    "    train_sampler = None\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=opt.batch_size, shuffle=(train_sampler is None),\n",
    "        num_workers=opt.num_workers, pin_memory=True, sampler=train_sampler)\n",
    "\n",
    "    return train_loader\n",
    "\n",
    "# opt = Config(\n",
    "#     dataset='cifar10',\n",
    "#     data_folder='dataset',\n",
    "#     batch_size=16,\n",
    "#     num_workers=4\n",
    "# )\n",
    "\n",
    "# train_loader = set_loader_contrastive(opt, crop_only=True)\n",
    "\n",
    "# (x1, x2), y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Encoder decoder:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, is_last=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        preact = out\n",
    "        out = F.relu(out)\n",
    "        if self.is_last:\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, is_last=False):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        preact = out\n",
    "        out = F.relu(out)\n",
    "        if self.is_last:\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, in_channel=3, zero_init_residual=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves\n",
    "        # like an identity. This improves the model by 0.2~0.3% according to:\n",
    "        # https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            stride = strides[i]\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, layer=100):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        feature_map = out\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        return feature_map, out\n",
    "\n",
    "\n",
    "def resnet18(**kwargs):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "\n",
    "\n",
    "def resnet34(**kwargs):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "\n",
    "def resnet50(**kwargs):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "\n",
    "def resnet101(**kwargs):\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "\n",
    "\n",
    "model_dict = {\n",
    "    'resnet18': [resnet18, 512],\n",
    "    'resnet34': [resnet34, 512],\n",
    "    'resnet50': [resnet50, 2048],\n",
    "    'resnet101': [resnet101, 2048],\n",
    "}\n",
    "\n",
    "\n",
    "class LinearBatchNorm(nn.Module):\n",
    "    \"\"\"Implements BatchNorm1d by BatchNorm2d, for SyncBN purpose\"\"\"\n",
    "    def __init__(self, dim, affine=True):\n",
    "        super(LinearBatchNorm, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.bn = nn.BatchNorm2d(dim, affine=affine)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.dim, 1, 1)\n",
    "        x = self.bn(x)\n",
    "        x = x.view(-1, self.dim)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SupConResNet(nn.Module):\n",
    "    \"\"\"backbone + projection head\"\"\"\n",
    "    def __init__(self, name='resnet50', head='mlp', feat_dim=128):\n",
    "        super(SupConResNet, self).__init__()\n",
    "        model_fun, dim_in = model_dict[name]\n",
    "        self.encoder = model_fun()\n",
    "        if head == 'linear':\n",
    "            self.head = nn.Linear(dim_in, feat_dim)\n",
    "        elif head == 'mlp':\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(dim_in, dim_in),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(dim_in, feat_dim)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'head not supported: {}'.format(head))\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_map, inner_feat = self.encoder(x)\n",
    "        # feat = F.normalize(self.head(feat), dim=1)\n",
    "        projection_feat = self.head(inner_feat)\n",
    "        return feature_map, inner_feat, projection_feat\n",
    "\n",
    "\n",
    "class SupCEResNet(nn.Module):\n",
    "    \"\"\"encoder + classifier\"\"\"\n",
    "    def __init__(self, name='resnet50', num_classes=10):\n",
    "        super(SupCEResNet, self).__init__()\n",
    "        model_fun, dim_in = model_dict[name]\n",
    "        self.encoder = model_fun()\n",
    "        self.fc = nn.Linear(dim_in, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.encoder(x))\n",
    "\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear classifier\"\"\"\n",
    "    def __init__(self, name='resnet50', num_classes=10):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        _, feat_dim = model_dict[name]\n",
    "        self.fc = nn.Linear(feat_dim, num_classes)\n",
    "\n",
    "    def forward(self, features):\n",
    "        return self.fc(features)\n",
    "    \n",
    "\n",
    "# model = SupConResNet(name='resnet50').to(device)\n",
    "\n",
    "# for x, y in tqdm(train_loader):\n",
    "#     x = x.to(device)\n",
    "#     feature_map, inner_feat, projection_feat = model(x)\n",
    "#     feature_map = feature_map.permute(0, 2, 3, 1)\n",
    "#     feature_map = feature_map.reshape(16, 16, 2048)\n",
    "#     preds_transform, preds_magnitude, preds_proba = decoder(feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_size, \n",
    "            vocab_size, \n",
    "            attention_dim, \n",
    "            encoder_dim, \n",
    "            decoder_dim,\n",
    "            num_transforms=4,\n",
    "            num_discrete_magnitude=11,\n",
    "            seq_length=10,\n",
    "            drop_prob=0.3\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        #save the model param\n",
    "        self.vocab_size = vocab_size\n",
    "        self.attention_dim = attention_dim\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "\n",
    "        self.num_transforms = num_transforms\n",
    "        self.num_discrete_magnitude = num_discrete_magnitude\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        \n",
    "        self.action_embd = nn.Embedding(3, embed_size)\n",
    "        self.branch_embd = nn.Embedding(2, embed_size)\n",
    "        \n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  \n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.lstm_cell = nn.LSTMCell(embed_size + encoder_dim, decoder_dim, bias=True)        \n",
    "        \n",
    "        self.fcn_transform = nn.Linear(decoder_dim,num_transforms)\n",
    "        self.fcn_magnitude = nn.Linear(decoder_dim,num_discrete_magnitude)\n",
    "\n",
    "        self.drop = nn.Dropout(drop_prob)\n",
    "    \n",
    "\n",
    "    def init_hidden_state(self, batch_size):\n",
    "        h = torch.zeros(batch_size, self.decoder_dim, device=device)\n",
    "        c = torch.zeros(batch_size, self.decoder_dim, device=device)\n",
    "        return h, c\n",
    "    \n",
    "\n",
    "    def forward(self, z1, z2):\n",
    "\n",
    "        #get the seq length to iterate\n",
    "        seq_length = self.seq_length\n",
    "        batch_size = z1.size(0)\n",
    "                \n",
    "        # Initialize LSTM state\n",
    "        h, c = self.init_hidden_state(batch_size)  # (batch_size, decoder_dim)\n",
    "        \n",
    "        preds_transform = torch.zeros(batch_size, 2, seq_length, self.num_transforms).to(device)\n",
    "        preds_magnitude = torch.zeros(batch_size, 2, seq_length, self.num_discrete_magnitude).to(device)\n",
    "\n",
    "        transform_action_id = torch.full((batch_size,), 0, dtype=torch.long, device=device)\n",
    "        magnitude_action_id = torch.full((batch_size,), 1, dtype=torch.long, device=device)\n",
    "\n",
    "        branch_id = torch.full((batch_size,2), 0, dtype=torch.long, device=device)\n",
    "        branch_id[:, 1] = 0\n",
    "\n",
    "        transform_action_embd = self.action_embd(transform_action_id)\n",
    "        magnitude_action_embd = self.action_embd(magnitude_action_id)\n",
    "        \n",
    "        features = [z1, z2]\n",
    "        \n",
    "        for branch in range(2):\n",
    "            for step in range(seq_length):\n",
    "\n",
    "                lstm_input = torch.cat((transform_action_embd, features[branch]), dim=-1)\n",
    "                h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "                output_transform = self.fcn_transform(self.drop(h))\n",
    "\n",
    "                lstm_input = torch.cat((magnitude_action_embd, features[branch]), dim=-1)\n",
    "                h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "                output_magnitude = self.fcn_magnitude(self.drop(h))\n",
    "                \n",
    "                preds_transform[:, branch, step] = output_transform\n",
    "                preds_magnitude[:, branch, step] = output_magnitude\n",
    "        \n",
    "        return preds_transform, preds_magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMS_DICT = [\n",
    "    ('brightness', vision_F.adjust_brightness, (0.1, 1.9)),\n",
    "    ('contrast', vision_F.adjust_contrast, (0.1, 1.9)),\n",
    "    ('saturation', vision_F.adjust_saturation, (0.1, 1.9)),\n",
    "    ('hue', vision_F.adjust_hue, (-0.45, 0.45)),\n",
    "]\n",
    "\n",
    "def get_transforms_list(actions_transform, actions_magnitude):\n",
    "\n",
    "    all_transform_lists = []\n",
    "    for i in range(actions_transform.size(0)):\n",
    "        for branch in range(actions_transform.size(1)):\n",
    "            transform_list = []\n",
    "            for s in range(actions_transform.size(2)):\n",
    "                transform_id = actions_transform[i, branch, s].item()\n",
    "                magnitude_id = actions_magnitude[i, branch, s].item()\n",
    "                func_name, func, (lower, upper) = TRANSFORMS_DICT[transform_id]\n",
    "                step = (upper - lower) / 10\n",
    "                magnitude = np.arange(start=lower, stop=upper+step, step=step)[magnitude_id]\n",
    "                transform_list.append((func_name, func, round(magnitude, 5)))\n",
    "            all_transform_lists.append(transform_list)\n",
    "\n",
    "    return all_transform_lists\n",
    "\n",
    "\n",
    "def apply_transformations(x1, x2, transform_list):\n",
    "\n",
    "    num_samples = x1.size(0)\n",
    "    stored_imgs = torch.zeros((2, num_samples, 3, 32, 32))\n",
    "\n",
    "    transform_i = 0\n",
    "    for i in range(x1.size(0)):\n",
    "        img1, img2 = x1[i], x2[i]\n",
    "        imgs = [img1, img2]\n",
    "        for branch in range(2):\n",
    "            img = imgs[branch]\n",
    "            # print('-----')\n",
    "            for transform_name, transform_func, magnitude in transform_list[transform_i]:\n",
    "                # print(transform_name, magnitude)\n",
    "                # print('before:',img.max())\n",
    "                img = transform_func(img, magnitude)\n",
    "                # print('after:',img.max())\n",
    "            stored_imgs[branch, i] = img\n",
    "\n",
    "    new_x1, new_x2 = stored_imgs[0], stored_imgs[1]\n",
    "    return new_x1, new_x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastivePPO:\n",
    "    def __init__(\n",
    "            self, \n",
    "            data_loader,\n",
    "            encoder, \n",
    "            decoder, \n",
    "            batch_size, \n",
    "            num_samples, \n",
    "            update_epochs\n",
    "        ):\n",
    "        \n",
    "        self.data_loader = data_loader\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples = num_samples\n",
    "        self.update_epochs = update_epochs\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.decoder.parameters(),\n",
    "            lr=100\n",
    "        )\n",
    "\n",
    "\n",
    "    def collect_samples(self):\n",
    "\n",
    "        batch_size = 128\n",
    "        num_samples = 128*4\n",
    "        decoder = self.decoder\n",
    "        encoder = self.encoder\n",
    "        data_loader = self.data_loader\n",
    "        encoder_dim = self.decoder.encoder_dim\n",
    "        \n",
    "        stored_z1 = torch.zeros((num_samples, encoder_dim))\n",
    "        stored_z2 = torch.zeros((num_samples, encoder_dim))\n",
    "        stored_preds_transform = torch.zeros((num_samples, 2, decoder.seq_length, decoder.num_transforms))\n",
    "        stored_preds_magnitude = torch.zeros((num_samples, 2, decoder.seq_length, decoder.num_discrete_magnitude))\n",
    "        stored_actions_transform = torch.zeros((num_samples, 2, decoder.seq_length, 1), dtype=torch.long)\n",
    "        stored_actions_magnitude = torch.zeros((num_samples, 2, decoder.seq_length, 1), dtype=torch.long)\n",
    "        stored_similarities = torch.zeros((num_samples,))\n",
    "\n",
    "        data_loader_iterator = iter(data_loader)\n",
    "\n",
    "        # for batch_i in tqdm(range(math.ceil(num_samples / batch_size))):\n",
    "        for batch_i in range(math.ceil(num_samples / batch_size)):\n",
    "\n",
    "            begin, end = batch_i*batch_size, (batch_i+1)*batch_size\n",
    "\n",
    "            (x1, x2), y = next(data_loader_iterator)\n",
    "\n",
    "            # print(x1.max(), x2.max())\n",
    "\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # _, _, z1 = encoder(x1)\n",
    "                # _, _, z2 = encoder(x2)\n",
    "                _, z1 = encoder(x1)\n",
    "                _, z2 = encoder(x2)\n",
    "\n",
    "            x1 = x1.cpu()\n",
    "            x2 = x2.cpu()\n",
    "\n",
    "            # print(z1.shape)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_transform, preds_magnitude = decoder(z1, z2)\n",
    "\n",
    "            actions_transform = preds_transform.argmax(dim=-1).unsqueeze(-1)\n",
    "            actions_magnitude = preds_magnitude.argmax(dim=-1).unsqueeze(-1)\n",
    "\n",
    "\n",
    "            # todo: transform actions to list of transforms\n",
    "            transforms_list = get_transforms_list(actions_transform, actions_magnitude)\n",
    "\n",
    "            # print(transforms_list)\n",
    "            \n",
    "            # todo: apply transformations on images\n",
    "            new_x1, new_x2 = apply_transformations(x1, x2, transforms_list)\n",
    "\n",
    "            # todo: pass transformed images into the encoder and get the new z1 and z2\n",
    "            new_x1 = new_x1.to(device)\n",
    "            new_x2 = new_x2.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # _, _, new_z1 = encoder(new_x1)\n",
    "                # _, _, new_z2 = encoder(new_x2)\n",
    "                _, new_z1 = encoder(new_x1)\n",
    "                _, new_z2 = encoder(new_x2)\n",
    "\n",
    "            # print('new_z1:', new_z1.max(), new_z2.max(), x1.max(), x2.max(), new_x1.max(), new_x2.max())\n",
    "            norm_z1, norm_z2 = F.normalize(new_z1), F.normalize(new_z2)\n",
    "            cosine_similarity = (norm_z1 * norm_z2).sum(dim=-1)\n",
    "\n",
    "            stored_z1[begin:end] = z1.detach().cpu()\n",
    "            stored_z2[begin:end] = z2.detach().cpu()\n",
    "            stored_preds_transform[begin:end] = preds_transform.log_softmax(dim=-1).detach().cpu()\n",
    "            stored_preds_magnitude[begin:end] = preds_magnitude.log_softmax(dim=-1).detach().cpu()\n",
    "            stored_actions_transform[begin:end] = actions_transform.detach().cpu()\n",
    "            stored_actions_magnitude[begin:end] = actions_magnitude.detach().cpu()\n",
    "            stored_similarities[begin:end] = cosine_similarity.detach().cpu()\n",
    "    \n",
    "        return (\n",
    "            (stored_z1, stored_z2), \n",
    "            (stored_preds_transform, stored_preds_magnitude),\n",
    "            (stored_actions_transform, stored_actions_magnitude),\n",
    "            stored_similarities\n",
    "        )\n",
    "\n",
    "    def ppo_update(self, stored_samples):\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "        num_samples = self.num_samples\n",
    "        decoder = self.decoder\n",
    "\n",
    "        (\n",
    "            (stored_z1, stored_z2), \n",
    "            (stored_preds_transform, stored_preds_magnitude),\n",
    "            (stored_actions_transform, stored_actions_magnitude),\n",
    "            stored_similarities\n",
    "        ) = stored_samples\n",
    "\n",
    "\n",
    "        for batch_i in range(math.ceil(num_samples / batch_size)):\n",
    "            begin, end = batch_i*batch_size, (batch_i+1)*batch_size\n",
    "            \n",
    "            z1, z2 = stored_z1[begin:end].to(device), stored_z2[begin:end].to(device)\n",
    "            old_preds_transform = stored_preds_transform[begin:end].to(device)\n",
    "            old_preds_magnitude = stored_preds_magnitude[begin:end].to(device)\n",
    "            old_actions_transform = stored_actions_transform[begin:end].to(device)\n",
    "            old_actions_magnitude = stored_actions_magnitude[begin:end].to(device)\n",
    "            reward = -1. * stored_similarities[begin:end].to(device)\n",
    "\n",
    "            # print('reward:', reward)\n",
    "            new_preds_transform, new_preds_magnitude = decoder(z1, z2)\n",
    "            # print('new_preds_transform:', new_preds_transform.mean())\n",
    "            new_preds_transform = new_preds_transform.log_softmax(dim=-1)\n",
    "            new_preds_magnitude = new_preds_magnitude.log_softmax(dim=-1)\n",
    "            # print('new_preds_transform log_softmax:', new_preds_transform.mean())\n",
    "\n",
    "            del z1, z2\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "            old_log_pi = torch.concatenate((\n",
    "                old_preds_transform.gather(-1, old_actions_transform),\n",
    "                old_preds_magnitude.gather(-1, old_actions_magnitude)\n",
    "            ), dim=-1)\n",
    "\n",
    "            new_log_pi = torch.concatenate((\n",
    "                new_preds_transform.gather(-1, old_actions_transform),\n",
    "                new_preds_magnitude.gather(-1, old_actions_magnitude)\n",
    "            ), dim=-1)\n",
    "\n",
    "            old_log_pi = torch.exp(old_log_pi.reshape(batch_size, -1).sum(dim=-1))\n",
    "            new_log_pi = torch.exp(new_log_pi.reshape(batch_size, -1).sum(dim=-1))\n",
    "\n",
    "            advantage = reward\n",
    "            ratio = torch.exp(new_log_pi - old_log_pi.detach())\n",
    "\n",
    "            # print('new_log_pi:', new_log_pi.mean())\n",
    "            # print('old_log_pi:', old_log_pi.mean())\n",
    "            # print('ratio:', ratio.mean())\n",
    "            # print('reward:', reward.mean())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1-0.1, 1+0.1) * advantage\n",
    "\n",
    "            print('surr1:', surr1.mean())\n",
    "            print('surr2:', surr2.mean())\n",
    "\n",
    "            loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0315, -0.0118, -0.0361, -0.0127, -0.0012,  0.0372, -0.0295, -0.0263,\n",
       "         0.0126, -0.0414, -0.0098], requires_grad=True)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoder = SupConResNet(name='resnet18').to(device)\n",
    "encoder = build_resnet18()\n",
    "encoder.load_state_dict(torch.load('resnet18-f37072fd.pth'))\n",
    "\n",
    "decoder = DecoderRNN(\n",
    "    embed_size=300,\n",
    "    vocab_size=10,\n",
    "    attention_dim=256,\n",
    "    encoder_dim=512,\n",
    "    decoder_dim=512,\n",
    "    num_transforms=4,\n",
    "    num_discrete_magnitude=11,\n",
    "    seq_length=1\n",
    ").to(device)\n",
    "\n",
    "opt = Config(\n",
    "    dataset='cifar10',\n",
    "    data_folder='dataset',\n",
    "    batch_size=128,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "data_loader = set_loader_contrastive(opt, crop_only=True)\n",
    "\n",
    "obj = ContrastivePPO(\n",
    "    data_loader=data_loader,\n",
    "    encoder=encoder, \n",
    "    decoder=decoder, \n",
    "    batch_size=128, \n",
    "    num_samples=128, \n",
    "    update_epochs=1,\n",
    ")\n",
    "\n",
    "list(decoder.parameters())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0315, -0.0118, -0.0361, -0.0127, -0.0012,  0.0372, -0.0295, -0.0263,\n",
       "         0.0126, -0.0414, -0.0098], requires_grad=True)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(decoder.parameters())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<01:19,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.4419, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9116, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9116, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:01<01:19,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.8176, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9711, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9711, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:02<01:16,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.8914, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9828, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9828, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:03<01:14,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.8568, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9773, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9773, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:03<01:12,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.9160, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9867, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:04<01:10,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.9012, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9844, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9844, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:05<01:09,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.9210, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9875, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:06<01:09,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.9160, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9867, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:06<01:09,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.9062, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9852, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:07<01:10,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.8914, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9828, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9828, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:08<01:10,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.8963, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9836, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:09<01:09,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.8897, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9820, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9820, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:10<01:08,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.9210, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9875, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:10<01:07,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.8765, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9805, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9805, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:11<01:06,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.8963, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9836, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:12<01:05,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.9214, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9879, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9879, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [00:13<01:04,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.9259, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9883, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [00:13<01:02,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.9309, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9891, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9891, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [00:14<01:01,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.9012, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9844, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9844, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [00:15<01:01,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.8864, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9820, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9820, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:16<01:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.9160, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9867, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [00:16<00:59,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.9160, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9867, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [00:17<00:59,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.9062, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9852, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [00:18<00:58,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.8963, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9836, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [00:19<00:57,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.9160, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9867, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [00:20<00:57,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.8765, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9805, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9805, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [00:20<00:57,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.9210, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9875, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [00:21<00:55,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.9111, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9859, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [00:22<00:55,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.9111, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9859, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:23<00:53,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.9060, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9850, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:24<00:55,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-0.7682, grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-0.9636, grad_fn=<MeanBackward0>)\n",
      "tensor(0.9636, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [00:25<01:07,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "tensor(1., grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [00:26<01:15,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "tensor(1., grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [00:28<01:15,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "tensor(1., grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [00:29<01:09,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "tensor(1., grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [00:30<01:16,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "tensor(1., grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [00:31<01:19,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "tensor(1., grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [00:33<01:21,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "tensor(1., grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [00:34<01:22,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "tensor(1., grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [00:35<01:17,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "tensor(1., grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [00:37<01:19,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "tensor(1., grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [00:38<01:19,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "tensor(1., grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [00:40<01:19,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "tensor(1., grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [00:41<01:19,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "tensor(1., grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [00:42<01:09,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "tensor(1., grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [00:43<01:02,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "tensor(1., grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [00:45<01:05,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "tensor(1., grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [00:46<01:07,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "tensor(1., grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [00:47<01:08,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "tensor(1., grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [00:49<01:08,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surr1: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "surr2: tensor(-1., grad_fn=<MeanBackward0>)\n",
      "tensor(1., grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [00:49<00:49,  1.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/nazim/Script/RL_contrastive/test_transform.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nazim/Script/RL_contrastive/test_transform.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m)):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/nazim/Script/RL_contrastive/test_transform.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     samples \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49mcollect_samples()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nazim/Script/RL_contrastive/test_transform.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     loss \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mppo_update(samples)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nazim/Script/RL_contrastive/test_transform.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# print(samples[-1])\u001b[39;00m\n",
      "\u001b[1;32m/home/nazim/Script/RL_contrastive/test_transform.ipynb Cell 17\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nazim/Script/RL_contrastive/test_transform.ipynb#X24sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nazim/Script/RL_contrastive/test_transform.ipynb#X24sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39m# _, _, z1 = encoder(x1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nazim/Script/RL_contrastive/test_transform.ipynb#X24sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39m# _, _, z2 = encoder(x2)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nazim/Script/RL_contrastive/test_transform.ipynb#X24sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     _, z1 \u001b[39m=\u001b[39m encoder(x1)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/nazim/Script/RL_contrastive/test_transform.ipynb#X24sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     _, z2 \u001b[39m=\u001b[39m encoder(x2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nazim/Script/RL_contrastive/test_transform.ipynb#X24sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m x1 \u001b[39m=\u001b[39m x1\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nazim/Script/RL_contrastive/test_transform.ipynb#X24sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m x2 \u001b[39m=\u001b[39m x2\u001b[39m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Script/RL_contrastive/utils/networks.py:131\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 131\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m~/Script/RL_contrastive/utils/networks.py:119\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    116\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m    117\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[0;32m--> 119\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer1(x)\n\u001b[1;32m    120\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[1;32m    121\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     90\u001b[0m     identity \u001b[39m=\u001b[39m x\n\u001b[0;32m---> 92\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m     93\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for _ in tqdm(range(100)):\n",
    "\n",
    "    samples = obj.collect_samples()\n",
    "    loss = obj.ppo_update(samples)\n",
    "    # print(samples[-1])\n",
    "    print(loss, samples[-1].mean())\n",
    "    # print(list(decoder.parameters())[0][0][:10])\n",
    "\n",
    "    # print(\n",
    "    #     loss, \n",
    "    #     round(samples[-1].mean().item(), 4)\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5526, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
