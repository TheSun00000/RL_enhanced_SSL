{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'custom_crop' from 'utils.transforms' (/home/nazim/Script/RL_contrastive/utils/transforms.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnetworks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecoderNoInput, DecoderRNN\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     apply_transformations,\n\u001b[1;32m      9\u001b[0m     get_transforms_list,\n\u001b[1;32m     10\u001b[0m     split_interval,\n\u001b[1;32m     11\u001b[0m     custom_crop\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     18\u001b[0m cifar10_dataset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCIFAR10(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'custom_crop' from 'utils.transforms' (/home/nazim/Script/RL_contrastive/utils/transforms.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as vision_F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.networks import DecoderNoInput, DecoderRNN\n",
    "from utils.transforms import (\n",
    "    apply_transformations,\n",
    "    get_transforms_list,\n",
    "    split_interval,\n",
    "    custom_crop\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "cifar10_dataset = torchvision.datasets.CIFAR10('dataset', download=True)\n",
    "\n",
    "\n",
    "class MyDatset(Dataset):\n",
    "    def __init__(self, train_dataset):\n",
    "        self.train_dataset = train_dataset\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.train_dataset)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        img, y = self.train_dataset[i][0], self.train_dataset[i][1]\n",
    "        x = self.to_tensor(img)\n",
    "        return x, y\n",
    "    \n",
    "    \n",
    "dataset = MyDatset(cifar10_dataset)\n",
    "img = dataset[4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(n, min, max):\n",
    "    if n > max:\n",
    "        return max\n",
    "    elif n < min:\n",
    "        return min\n",
    "    return n\n",
    "\n",
    "def custom_crop(img, position, scale):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        position (_type_): in \n",
    "            [0, 1, 2,\n",
    "             3, 4, 5,\n",
    "             6, 7, 8]\n",
    "        target_area (float): in [0.08, 1.]\n",
    "    \"\"\"\n",
    "\n",
    "    position = clip(position, 0, 8)\n",
    "    \n",
    "    ratio = (3.0 / 4.0, 4.0 / 3.0)\n",
    "    \n",
    "    *_, height, width = img.shape\n",
    "    area = height * width\n",
    "\n",
    "    target_area = area * scale\n",
    "    log_ratio = torch.log(torch.tensor(ratio))\n",
    "    aspect_ratio = torch.exp(torch.empty(1).uniform_(log_ratio[0], log_ratio[1])).item()\n",
    "\n",
    "    \n",
    "    w = int(round((target_area * aspect_ratio)**0.5))\n",
    "    h = int(round((target_area / aspect_ratio)**0.5))\n",
    "    h = clip(h, 0, height)\n",
    "    w = clip(w, 0, width)\n",
    "    \n",
    "    i, j = height * (position//3)/3, width * (position%3)/3\n",
    "    i_noise = torch.empty(1).uniform_(0, height/3).item()\n",
    "    j_noise = torch.empty(1).uniform_(0, width/3).item()    \n",
    "    i, j = int(round(i + i_noise)), int(round(j + j_noise))\n",
    "    \n",
    "    if i + h > height:\n",
    "        i = height - h\n",
    "    if j + w > width:\n",
    "        j = width - w \n",
    "    \n",
    "    return vision_F.resized_crop(img, i, j, h, w, size=(height, width), antialias=\"warn\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(img.permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cropped_img = custom_crop(\n",
    "    img=img, \n",
    "    position=5, \n",
    "    scale=0.2\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(cropped_img.permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2, 0.289, 0.378, 0.467, 0.556, 0.644, 0.733, 0.822, 0.911, 1.0]"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_interval(0.2, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torchvision.models import resnet18, resnet50\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "            num_transforms,\n",
    "            num_discrete_magnitude,\n",
    "            device\n",
    "            ):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.num_transforms = num_transforms\n",
    "        self.num_discrete_magnitude = num_discrete_magnitude\n",
    "        self.seq_length = num_transforms\n",
    "        self.device = device\n",
    "        \n",
    "        self.permutations = torch.tensor(\n",
    "            list(permutations(range(4)))\n",
    "            ).to(device)\n",
    "        \n",
    "        self.num_transforms_permutations = len(self.permutations)\n",
    "        self.num_actions = num_transforms * num_discrete_magnitude\n",
    "        \n",
    "        self.output_dim_per_view = (\n",
    "            # Crop (Position[10], Area[10]):\n",
    "            num_discrete_magnitude + num_discrete_magnitude + \\\n",
    "            # Color (4*magnitude[10], permutations):\n",
    "            4 * num_discrete_magnitude + self.num_transforms_permutations + \\\n",
    "            # Gray (Porba[10]):\n",
    "            num_discrete_magnitude + \\\n",
    "            # Gaussian blur (Sigma[10], Proba[10]):\n",
    "            num_discrete_magnitude + num_discrete_magnitude\n",
    "        )\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(128, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2 * self.output_dim_per_view)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x, old_action_index=None):\n",
    "        \n",
    "        *leading_dim, input_dim = x.shape                        \n",
    "        output = self.model(x)\n",
    "        \n",
    "        D = self.num_discrete_magnitude\n",
    "        crop_position_offset = 0\n",
    "        crop_area_offset = crop_position_offset + 2*D\n",
    "        color_magnitude_offset = crop_area_offset + 2*D\n",
    "        color_permutation_offset = color_magnitude_offset + 2*(4*D)\n",
    "        gray_proba_offset = color_permutation_offset + 2*self.num_transforms_permutations\n",
    "        blur_sigma_offset = gray_proba_offset + 2*D\n",
    "        blur_proba_offset = blur_sigma_offset + 2*D\n",
    "                \n",
    "        \n",
    "        crop_position_logits = output[:, :crop_area_offset]\n",
    "        crop_area_logits = output[:, crop_area_offset:color_magnitude_offset]\n",
    "        color_magnitude_logits = output[:, color_magnitude_offset:color_permutation_offset]\n",
    "        color_permutation_logits = output[:, color_permutation_offset:gray_proba_offset]\n",
    "        gray_proba_logits = output[:, gray_proba_offset:blur_sigma_offset]\n",
    "        blur_sigma_logits = output[:, blur_sigma_offset:blur_proba_offset]\n",
    "        blur_proba_logits = output[:, blur_proba_offset:]\n",
    "\n",
    "        \n",
    "        crop_position_logits = crop_position_logits.reshape(*leading_dim, 2, D)\n",
    "        crop_area_logits = crop_area_logits.reshape(*leading_dim, 2, D)\n",
    "        color_magnitude_logits = color_magnitude_logits.reshape(*leading_dim, 2, 4, D)\n",
    "        color_permutation_logits = color_permutation_logits.reshape(*leading_dim, 2, self.num_transforms_permutations)\n",
    "        gray_proba_logits = gray_proba_logits.reshape(*leading_dim, 2, D)\n",
    "        blur_sigma_logits = blur_sigma_logits.reshape(*leading_dim, 2, D)\n",
    "        blur_proba_logits = blur_proba_logits.reshape(*leading_dim, 2, D)\n",
    "        \n",
    "        \n",
    "        crop_position_dist = torch.distributions.Categorical(logits=crop_position_logits)\n",
    "        crop_area_dist = torch.distributions.Categorical(logits=crop_area_logits)\n",
    "        color_magnitude_dist = torch.distributions.Categorical(logits=color_magnitude_logits)\n",
    "        color_permutation_dist = torch.distributions.Categorical(logits=color_permutation_logits)\n",
    "        gray_proba_dist = torch.distributions.Categorical(logits=gray_proba_logits)\n",
    "        blur_sigma_dist = torch.distributions.Categorical(logits=blur_sigma_logits)\n",
    "        blur_proba_dist = torch.distributions.Categorical(logits=blur_proba_logits)\n",
    "                \n",
    "        \n",
    "        if old_action_index is None:\n",
    "            crop_position_index = crop_position_dist.sample()\n",
    "            crop_area_index = crop_area_dist.sample()\n",
    "            color_magnitude_index = color_magnitude_dist.sample()\n",
    "            color_permutation_index = color_permutation_dist.sample()\n",
    "            gray_proba_index = gray_proba_dist.sample()\n",
    "            blur_sigma_index = blur_sigma_dist.sample()\n",
    "            blur_proba_index = blur_proba_dist.sample()\n",
    "        else:\n",
    "            crop_position_index = old_action_index[..., 0]\n",
    "            crop_area_index = old_action_index[..., 1]\n",
    "            color_magnitude_index = old_action_index[..., 2:6]\n",
    "            color_permutation_index = old_action_index[..., 6]\n",
    "            gray_proba_index = old_action_index[..., 7]\n",
    "            blur_sigma_index = old_action_index[..., 8]\n",
    "            blur_proba_index = old_action_index[..., 9]\n",
    "        \n",
    "        \n",
    "        crop_position_log_p = F.log_softmax(crop_position_logits, dim=-1).gather(-1, crop_position_index.unsqueeze(-1)).reshape(*leading_dim, -1).sum(-1, keepdim=True)\n",
    "        crop_area_log_p = F.log_softmax(crop_area_logits, dim=-1).gather(-1, crop_area_index.unsqueeze(-1)).reshape(*leading_dim, -1).sum(-1, keepdim=True)\n",
    "        color_magnitude_log_p = F.log_softmax(color_magnitude_logits, dim=-1).gather(-1, color_magnitude_index.unsqueeze(-1)).reshape(*leading_dim, -1).sum(-1, keepdim=True)\n",
    "        color_permutation_log_p = F.log_softmax(color_permutation_logits, dim=-1).gather(-1, color_permutation_index.unsqueeze(-1)).reshape(*leading_dim, -1).sum(-1, keepdim=True)\n",
    "        gray_proba_log_p = F.log_softmax(gray_proba_logits, dim=-1).gather(-1, gray_proba_index.unsqueeze(-1)).reshape(*leading_dim, -1).sum(-1, keepdim=True)\n",
    "        blur_sigma_log_p = F.log_softmax(blur_sigma_logits, dim=-1).gather(-1, blur_sigma_index.unsqueeze(-1)).reshape(*leading_dim, -1).sum(-1, keepdim=True)\n",
    "        blur_proba_log_p = F.log_softmax(blur_proba_logits, dim=-1).gather(-1, blur_proba_index.unsqueeze(-1)).reshape(*leading_dim, -1).sum(-1, keepdim=True)\n",
    "\n",
    "\n",
    "        log_p = (crop_position_log_p + crop_area_log_p) + (color_magnitude_log_p + color_permutation_log_p) + (gray_proba_log_p) + (blur_sigma_log_p + blur_proba_log_p)\n",
    "        \n",
    "        actions_index = torch.concat((\n",
    "            crop_position_index.unsqueeze(-1),\n",
    "            crop_area_index.unsqueeze(-1),\n",
    "            color_magnitude_index,\n",
    "            color_permutation_index.unsqueeze(-1),\n",
    "            gray_proba_index.unsqueeze(-1),\n",
    "            blur_sigma_index.unsqueeze(-1),\n",
    "            blur_proba_index.unsqueeze(-1),\n",
    "        ), dim=-1)\n",
    "                \n",
    "        entropy = (crop_position_dist.entropy().mean() + crop_area_dist.entropy().mean()) + (color_magnitude_dist.entropy().mean() + color_permutation_dist.entropy().mean()) + (gray_proba_dist.entropy().mean()) + (blur_sigma_dist.entropy().mean() + blur_proba_dist.entropy().mean())\n",
    "        \n",
    "        return (\n",
    "                log_p,\n",
    "                actions_index,\n",
    "                entropy\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True), tensor(True), tensor(True))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = DecoderRNN(4 ,10, 'cpu')\n",
    "\n",
    "x = torch.rand((256, 128)).to('cpu')\n",
    "log_p, actions_index, entropy = decoder(x)\n",
    "new_log_p, new_actions_index, new_entropy = decoder(x, old_action_index=actions_index)\n",
    "\n",
    "(log_p == new_log_p).all(), (actions_index == new_actions_index).all(), (entropy == new_entropy).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_brightness(img, *args):\n",
    "    magnitude = args[0]\n",
    "    return vision_F.adjust_brightness(img, magnitude)\n",
    "\n",
    "def adjust_contrast(img, *args):\n",
    "    magnitude = args[0]\n",
    "    return vision_F.adjust_contrast(img, magnitude)\n",
    "\n",
    "def adjust_saturation(img, *args):\n",
    "    magnitude = args[0]\n",
    "    return vision_F.adjust_saturation(img, magnitude)\n",
    "\n",
    "def adjust_hue(img, *args):\n",
    "    magnitude = args[0]\n",
    "    return vision_F.adjust_hue(img, magnitude)\n",
    "\n",
    "def crop_image(img, *args):\n",
    "    position, scale = args\n",
    "    return custom_crop(img, position, scale)\n",
    "\n",
    "def blur_image(img, *args):\n",
    "    sigma, proba = args\n",
    "    if torch.rand(1).item() < proba:\n",
    "        return vision_F.gaussian_blur(img, kernel_size=(3, 3), sigma=sigma)\n",
    "    return img\n",
    "\n",
    "def grayscale_image(img, *args):\n",
    "    proba = args[0]\n",
    "    if torch.rand(1).item() < proba:\n",
    "        return vision_F.rgb_to_grayscale(img, num_output_channels=3)\n",
    "    return img\n",
    "\n",
    "def flip_image(img, *args):\n",
    "    return transforms.RandomHorizontalFlip(p=0.5)(img)\n",
    "\n",
    "\n",
    "TRANSFORMS_DICT = {\n",
    "    'brightness': {\n",
    "        'function':adjust_brightness,\n",
    "        'magnitude':((0.6, 1.4))\n",
    "    },\n",
    "    'contrast': {\n",
    "        'function':adjust_contrast,\n",
    "        'magnitude':((0.6, 1.4))\n",
    "    },\n",
    "    'saturation': {\n",
    "        'function':adjust_saturation,\n",
    "        'magnitude':((0.6, 1.4))\n",
    "    },\n",
    "    'hue': {\n",
    "        'function':adjust_hue,\n",
    "        'magnitude':((-0.1, 0.1))\n",
    "    },\n",
    "    'crop': {\n",
    "        'function':custom_crop,\n",
    "        'position':None,\n",
    "        'area':(0.2, 1)\n",
    "    },\n",
    "    'gray': {\n",
    "        'function':grayscale_image,\n",
    "        'proba':(0, 1)\n",
    "    },\n",
    "    'blur': {\n",
    "        'function':blur_image,\n",
    "        'proba':(0, 1),\n",
    "        'sigma':(0.1, 2)\n",
    "    },\n",
    "    'flip': {\n",
    "        'function':flip_image\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "permuatations = list(permutations(range(4)))\n",
    "\n",
    "\n",
    "def get_transforms_list_2(actions, num_magnitudes):\n",
    "    \n",
    "    all_transform_lists = []\n",
    "    for branch in range(actions.size(1)):\n",
    "        branch_transform_lists = []\n",
    "        \n",
    "        for i in range(actions.size(0)):\n",
    "            transform_list = []\n",
    "\n",
    "            crop_position_index = actions[i, branch, 0]\n",
    "            crop_area_index = actions[i, branch, 1]\n",
    "            color_magnitude_index = actions[i, branch, 2:6]\n",
    "            color_permutation_index = actions[i, branch, 6]\n",
    "            gray_proba_index = actions[i, branch, 7]\n",
    "            blur_sigma_index = actions[i, branch, 8]\n",
    "            blur_proba_index = actions[i, branch, 9]\n",
    "            \n",
    "            \n",
    "            areas_list = split_interval(TRANSFORMS_DICT['crop']['area'][0], TRANSFORMS_DICT['crop']['area'][1], num_magnitudes)\n",
    "            area = areas_list[crop_area_index]\n",
    "            position = crop_position_index.item()\n",
    "            transform_list.append(('crop', TRANSFORMS_DICT['crop']['function'], (position, area)))\n",
    "            \n",
    "            transform_list.append(('flip', TRANSFORMS_DICT['flip']['function'], ()))\n",
    "            \n",
    "            color_transformations = []\n",
    "            for color_distortion, magnitude_index in zip(['brightness', 'contrast', 'saturation', 'hue'], color_magnitude_index):\n",
    "                magnitudes_list = split_interval(TRANSFORMS_DICT[color_distortion]['magnitude'][0], TRANSFORMS_DICT[color_distortion]['magnitude'][1], num_magnitudes)\n",
    "                magnitude = magnitudes_list[magnitude_index]\n",
    "                color_transformations.append((color_distortion, TRANSFORMS_DICT[color_distortion]['function'], magnitude))\n",
    "            color_transformations = [ color_transformations[i] for i in permuatations[color_permutation_index]]\n",
    "            transform_list += color_transformations\n",
    "            \n",
    "            probas_list = split_interval(TRANSFORMS_DICT['gray']['proba'][0], TRANSFORMS_DICT['gray']['proba'][1], num_magnitudes)\n",
    "            proba = probas_list[gray_proba_index]\n",
    "            transform_list.append(('gray', TRANSFORMS_DICT['gray']['function'], proba))\n",
    "            \n",
    "            sigmas_list = split_interval(TRANSFORMS_DICT['blur']['sigma'][0], TRANSFORMS_DICT['blur']['sigma'][1], num_magnitudes)\n",
    "            probas_list = split_interval(TRANSFORMS_DICT['blur']['proba'][0], TRANSFORMS_DICT['blur']['proba'][1], num_magnitudes)\n",
    "            sigma = sigmas_list[blur_sigma_index]\n",
    "            proba = probas_list[blur_proba_index]\n",
    "            transform_list.append(('blur', TRANSFORMS_DICT['blur']['function'], (sigma, proba)))\n",
    "            \n",
    "            \n",
    "            branch_transform_lists.append(transform_list)\n",
    "    \n",
    "        all_transform_lists.append(branch_transform_lists)\n",
    "        \n",
    "    return all_transform_lists[0], all_transform_lists[1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transform_list1, transform_list2 = get_transforms_list(actions_index, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformations(img1, transform_list):\n",
    "\n",
    "    num_samples = img1.size(0)\n",
    "    stored_imgs = torch.zeros((num_samples, 3, 32, 32))\n",
    "\n",
    "    for i in range(img1.size(0)):\n",
    "        img = img1[i]\n",
    "        # print('----', img.min(), img.max())\n",
    "        for (transform_name, transform_func, args) in transform_list[i]:\n",
    "            print(transform_name, args)\n",
    "            img = transform_func(img, *args)\n",
    "            # print('----', transform_name, img.min(), img.max())\n",
    "        stored_imgs[i] = img\n",
    "\n",
    "    return stored_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crop (2, 0.378)\n",
      "flip ()\n",
      "brightness 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nazim/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Value after * must be an iterable, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[225], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mapply_transformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_list1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[224], line 11\u001b[0m, in \u001b[0;36mapply_transformations\u001b[0;34m(img1, transform_list)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (transform_name, transform_func, args) \u001b[38;5;129;01min\u001b[39;00m transform_list[i]:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(transform_name, args)\n\u001b[0;32m---> 11\u001b[0m     img \u001b[38;5;241m=\u001b[39m transform_func(img, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# print('----', transform_name, img.min(), img.max())\u001b[39;00m\n\u001b[1;32m     13\u001b[0m stored_imgs[i] \u001b[38;5;241m=\u001b[39m img\n",
      "\u001b[0;31mTypeError\u001b[0m: Value after * must be an iterable, not float"
     ]
    }
   ],
   "source": [
    "apply_transformations(img.unsqueeze(0), transform_list1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
