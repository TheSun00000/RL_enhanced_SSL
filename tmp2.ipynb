{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "# from torchvision.models import resnet18, resnet50\n",
    "from utils.resnet import resnet18, resnet50\n",
    "from itertools import permutations\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cuda'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecoderNN_1input(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            transforms,\n",
    "            num_discrete_magnitude,\n",
    "            device\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        #save the model param\n",
    "        self.encoder_dim = 2028\n",
    "        self.decoder_dim = 512\n",
    "        self.embed_size = 128\n",
    "\n",
    "        self.transforms = transforms\n",
    "        num_transforms = len(transforms)\n",
    "        self.num_transforms = num_transforms\n",
    "        self.num_discrete_magnitude = num_discrete_magnitude\n",
    "        self.seq_length = 3\n",
    "\n",
    "        self.transform_embedding = nn.Embedding(num_transforms+1, self.embed_size)\n",
    "        self.magnitude_embedding = nn.Embedding(num_discrete_magnitude+1, self.embed_size)\n",
    "        self.branch_id_embedding = nn.Embedding(2, self.embed_size)\n",
    "        self.action_id_embedding = nn.Embedding(2, self.embed_size)\n",
    "\n",
    "        self.rnn = nn.LSTMCell(self.embed_size * self.seq_length * 2 * 2, self.decoder_dim, bias=True)\n",
    "        \n",
    "        self.transform_fc = nn.Linear(self.decoder_dim,num_transforms)\n",
    "        self.magnitude_fc = nn.Linear(self.decoder_dim,num_discrete_magnitude)\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "    \n",
    "\n",
    "    def init_hidden_state(self, batch_size):\n",
    "        h = torch.zeros(batch_size, self.decoder_dim, device=device)\n",
    "        c = torch.zeros(batch_size, self.decoder_dim, device=device)\n",
    "        return h, c\n",
    "    \n",
    "\n",
    "    def lstm_forward(self, transform_history, magnitude_history, h_t, c_t):\n",
    "        \n",
    "        batch_size = transform_history.shape[0]\n",
    "        \n",
    "        transform_history_embd = self.transform_embedding(transform_history)\n",
    "        magnitude_history_embd = self.magnitude_embedding(magnitude_history)\n",
    "        input = torch.concat(\n",
    "            (transform_history_embd, magnitude_history_embd),\n",
    "            dim=-1\n",
    "        ).reshape(batch_size, -1)\n",
    "        h_t, c_t = self.rnn(input, (h_t, c_t))\n",
    "        transform_logits = self.transform_fc(h_t)\n",
    "        magnitude_logits = self.magnitude_fc(h_t)\n",
    "        return h_t, c_t, transform_logits, magnitude_logits\n",
    "\n",
    "\n",
    "    def forward(self, batch_size, old_action=None):\n",
    "        \n",
    "        device = self.device\n",
    "        \n",
    "        if old_action is not None:\n",
    "            old_transform_actions_index = torch.zeros((batch_size, 2, self.seq_length), dtype=torch.long).to(device)\n",
    "            old_magnitude_actions_index = torch.zeros((batch_size, 2, self.seq_length), dtype=torch.long).to(device)\n",
    "            for i in range(len(old_action)):\n",
    "                for b in range(2):\n",
    "                    for s in range(self.seq_length):\n",
    "                        transform_id = self.transforms.index(old_action[i][b][s][0])\n",
    "                        level = old_action[i][b][s][2]\n",
    "                        magnitude_id = round(level * self.num_discrete_magnitude)\n",
    "                        old_transform_actions_index[i, b, s] = transform_id\n",
    "                        old_magnitude_actions_index[i, b, s] = magnitude_id\n",
    "            \n",
    "\n",
    "        \n",
    "        log_p =  torch.zeros(batch_size, 2, self.seq_length).to(device)\n",
    "        \n",
    "        transform_history = torch.full((batch_size, 2, self.seq_length), self.num_transforms, dtype=torch.long).to(device)\n",
    "        magnitude_history = torch.full((batch_size, 2, self.seq_length), self.num_discrete_magnitude, dtype=torch.long).to(device)\n",
    "\n",
    "        transform_entropy = 0\n",
    "        magnitude_entropy = 0\n",
    "        \n",
    "        # Initialize LSTM state\n",
    "        h_t, c_t = self.init_hidden_state(batch_size)  # (batch_size, decoder_dim)\n",
    "        \n",
    "        for branch in range(2):\n",
    "            \n",
    "            for step in range(self.seq_length):\n",
    "\n",
    "                h_t, c_t, transform_logits, magnitude_logits = self.lstm_forward(\n",
    "                    transform_history=transform_history,\n",
    "                    magnitude_history=magnitude_history,\n",
    "                    h_t=h_t,\n",
    "                    c_t=c_t,\n",
    "                )\n",
    "                if old_action is None:\n",
    "                    transform_action_index = Categorical(logits=transform_logits).sample()\n",
    "                    magnitude_action_index = Categorical(logits=magnitude_logits).sample()\n",
    "                else:\n",
    "                    transform_action_index = old_transform_actions_index[:, branch, step]\n",
    "                    magnitude_action_index = old_magnitude_actions_index[:, branch, step]\n",
    "                                \n",
    "                \n",
    "                transform_log_p = F.log_softmax(transform_logits, dim=-1).gather(-1,transform_action_index.unsqueeze(-1))\n",
    "                magnitude_log_p = F.log_softmax(magnitude_logits, dim=-1).gather(-1,magnitude_action_index.unsqueeze(-1))\n",
    "                \n",
    "                log_p[:, branch, step] = transform_log_p.squeeze(-1) + magnitude_log_p.squeeze(-1)\n",
    "                \n",
    "                transform_entropy += Categorical(logits=transform_logits).entropy().mean()\n",
    "                magnitude_entropy += Categorical(logits=transform_logits).entropy().mean()\n",
    "                \n",
    "                transform_history[:, branch, step] = transform_action_index\n",
    "                magnitude_history[:, branch, step] = magnitude_action_index\n",
    "\n",
    "\n",
    "\n",
    "        transform_entropy /= (2*self.seq_length)\n",
    "        magnitude_entropy /= (2*self.seq_length)\n",
    "        entropy = transform_entropy + magnitude_entropy\n",
    "        \n",
    "        log_p = log_p.reshape(batch_size, -1).sum(-1) \n",
    "        log_p = log_p.unsqueeze(-1)\n",
    "        \n",
    "        action = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            action.append([])\n",
    "            action[-1].append([])\n",
    "            action[-1].append([])\n",
    "            for b in range(2):\n",
    "                for s in range(self.seq_length):\n",
    "                    level = (magnitude_history[i, b, s] / self.num_discrete_magnitude).item()\n",
    "                    # level = magnitude_history[i, b, s].item()\n",
    "                    \n",
    "                    action[-1][b].append((\n",
    "                        self.transforms[transform_history[i, b, s]],\n",
    "                        0.8,\n",
    "                        level\n",
    "                    ))\n",
    "        \n",
    "        return (\n",
    "            log_p,\n",
    "            action,\n",
    "            entropy\n",
    "        )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 2, 3)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_action), len(old_action[0]), len(old_action[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('nazim', 0.8, 0.800000011920929),\n",
       "  ('kelba', 0.8, 0.20000000298023224),\n",
       "  ('pipipopo', 0.8, 0.10000000149011612)],\n",
       " [('nazim', 0.8, 0.0),\n",
       "  ('pipipopo', 0.8, 0.10000000149011612),\n",
       "  ('nazim', 0.8, 0.10000000149011612)]]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_action[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('nazim', 0.8, 0.30000001192092896),\n",
       "  ('pipipopo', 0.8, 0.6000000238418579),\n",
       "  ('kelba', 0.8, 0.800000011920929)],\n",
       " [('kelba', 0.8, 0.6000000238418579),\n",
       "  ('pipipopo', 0.8, 0.4000000059604645),\n",
       "  ('nazim', 0.8, 0.20000000298023224)]]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_action[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = ['nazim', 'pipipopo', 'kelba']\n",
    "net = DecoderNN_1input(transforms, 10, device).to(device)\n",
    "\n",
    "for _ in range(100):\n",
    "    old_log_p, old_action, old_entropy = net(batch_size=16)\n",
    "    new_log_p, new_action, new_entropy = net(batch_size=16, old_action=old_action)\n",
    "    \n",
    "    assert (old_log_p == new_log_p).all()\n",
    "    assert (old_entropy == new_entropy).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[('kelba', 0.8, 0.699999988079071), ('pipipopo', 0.8, 0.20000000298023224)],\n",
       "  [('nazim', 0.8, 0.6000000238418579), ('kelba', 0.8, 0.10000000149011612)]],\n",
       " [[('nazim', 0.8, 0.800000011920929), ('pipipopo', 0.8, 0.30000001192092896)],\n",
       "  [('kelba', 0.8, 0.30000001192092896), ('kelba', 0.8, 0.699999988079071)]],\n",
       " [[('kelba', 0.8, 0.6000000238418579), ('kelba', 0.8, 0.800000011920929)],\n",
       "  [('pipipopo', 0.8, 0.699999988079071),\n",
       "   ('pipipopo', 0.8, 0.6000000238418579)]],\n",
       " [[('kelba', 0.8, 0.10000000149011612), ('nazim', 0.8, 0.4000000059604645)],\n",
       "  [('nazim', 0.8, 0.699999988079071), ('nazim', 0.8, 0.9000000357627869)]],\n",
       " [[('nazim', 0.8, 0.800000011920929), ('nazim', 0.8, 0.30000001192092896)],\n",
       "  [('nazim', 0.8, 0.6000000238418579), ('pipipopo', 0.8, 0.6000000238418579)]],\n",
       " [[('nazim', 0.8, 0.9000000357627869), ('kelba', 0.8, 0.5)],\n",
       "  [('kelba', 0.8, 0.10000000149011612), ('kelba', 0.8, 0.10000000149011612)]],\n",
       " [[('nazim', 0.8, 0.20000000298023224), ('kelba', 0.8, 0.0)],\n",
       "  [('nazim', 0.8, 0.6000000238418579),\n",
       "   ('pipipopo', 0.8, 0.20000000298023224)]],\n",
       " [[('kelba', 0.8, 0.6000000238418579), ('kelba', 0.8, 0.5)],\n",
       "  [('kelba', 0.8, 0.4000000059604645), ('nazim', 0.8, 0.30000001192092896)]],\n",
       " [[('pipipopo', 0.8, 0.0), ('pipipopo', 0.8, 0.5)],\n",
       "  [('kelba', 0.8, 0.10000000149011612), ('kelba', 0.8, 0.699999988079071)]],\n",
       " [[('kelba', 0.8, 0.5), ('pipipopo', 0.8, 0.10000000149011612)],\n",
       "  [('nazim', 0.8, 0.800000011920929), ('nazim', 0.8, 0.800000011920929)]],\n",
       " [[('nazim', 0.8, 0.4000000059604645), ('kelba', 0.8, 0.20000000298023224)],\n",
       "  [('nazim', 0.8, 0.5), ('pipipopo', 0.8, 0.0)]],\n",
       " [[('nazim', 0.8, 0.5), ('pipipopo', 0.8, 0.5)],\n",
       "  [('pipipopo', 0.8, 0.30000001192092896), ('nazim', 0.8, 0.800000011920929)]],\n",
       " [[('kelba', 0.8, 0.5), ('pipipopo', 0.8, 0.20000000298023224)],\n",
       "  [('kelba', 0.8, 0.800000011920929), ('pipipopo', 0.8, 0.6000000238418579)]],\n",
       " [[('kelba', 0.8, 0.6000000238418579), ('nazim', 0.8, 0.9000000357627869)],\n",
       "  [('pipipopo', 0.8, 0.6000000238418579), ('kelba', 0.8, 0.4000000059604645)]],\n",
       " [[('nazim', 0.8, 0.10000000149011612), ('nazim', 0.8, 0.10000000149011612)],\n",
       "  [('kelba', 0.8, 0.10000000149011612), ('nazim', 0.8, 0.0)]],\n",
       " [[('nazim', 0.8, 0.6000000238418579), ('kelba', 0.8, 0.4000000059604645)],\n",
       "  [('pipipopo', 0.8, 0.4000000059604645),\n",
       "   ('kelba', 0.8, 0.20000000298023224)]]]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2],\n",
       "         [1, 0]],\n",
       "\n",
       "        [[0, 0],\n",
       "         [0, 2]],\n",
       "\n",
       "        [[0, 2],\n",
       "         [1, 2]],\n",
       "\n",
       "        [[2, 1],\n",
       "         [0, 2]],\n",
       "\n",
       "        [[1, 2],\n",
       "         [1, 2]],\n",
       "\n",
       "        [[1, 0],\n",
       "         [2, 1]],\n",
       "\n",
       "        [[2, 0],\n",
       "         [0, 1]],\n",
       "\n",
       "        [[1, 1],\n",
       "         [0, 2]],\n",
       "\n",
       "        [[1, 0],\n",
       "         [2, 2]],\n",
       "\n",
       "        [[0, 2],\n",
       "         [0, 2]],\n",
       "\n",
       "        [[2, 1],\n",
       "         [2, 1]],\n",
       "\n",
       "        [[2, 2],\n",
       "         [1, 0]],\n",
       "\n",
       "        [[1, 1],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0, 2],\n",
       "         [2, 1]],\n",
       "\n",
       "        [[1, 1],\n",
       "         [1, 0]],\n",
       "\n",
       "        [[0, 2],\n",
       "         [2, 0]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
